{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab lesson 4: Reinforcement Learning\n",
    "\n",
    "In this tutorial we will see some additional functionalities available to OpenAI Gym environments\n",
    "\n",
    "## Cliff Environment\n",
    "The environment used is **Cliff** (taken from the book of Sutton and Barto as visible in the figure)\n",
    "\n",
    "![Cliff](images/cliff.png)\n",
    "\n",
    "The agent starts in cell $(3, 0)$ and has to reach the goal in $(3, 11)$. Falling from the cliff resets the position to the start state (the episode ends only when the goal state is reached). All other cells are safe. Action dinamycs is deterministic, meaning that the agent always reaches the desired next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env = gym.make(\"Cliff-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell types are the following:\n",
    "\n",
    "- x - Start position\n",
    "- o - Safe\n",
    "- C - Cliff\n",
    "- T - Goal\n",
    "\n",
    "Rewards:\n",
    "\n",
    "- -1 for each \"safe\" cell (o)\n",
    "- -100 for falling from the cliff (C)\n",
    "\n",
    "In addition to the functionalities of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- step(action): the agent performs action from the current state. Returns a tuple (new_state, reward, done, info) where:\n",
    "    - new_state: is the new state reached as a consequence of the agent's last action\n",
    "    - reward: the reward obtained by the agent\n",
    "    - done: True if the episode has ended, False otherwise\n",
    "    - info: not used, you can safely discard it\n",
    "\n",
    "- reset(): the environment is reset and the agent goes back to the starting position. Returns the initial state id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "action = 0 # Go UP\n",
    "new_state, reward, done, _ = env.step(0)\n",
    "\n",
    "print(\"State: {} \\nAction: {} \\nNew State: {} \\nReward: {} \\nDone: {}\".format(state, action, new_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to execute a random policy in the environment: we create such policy as usual, we reset the environment to its initial state and also set a maximum number of steps for the episod. Then we execute a loop where at each iteration a step is performed by using the action defined by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
    "state = env.reset()\n",
    "ep_limit = 20\n",
    "\n",
    "el = 0\n",
    "total_reward = 0\n",
    "\n",
    "# Episode execution loop\n",
    "for _ in range(ep_limit):\n",
    "    next_state, reward, done, _ = env.step(policy[state])  # Execute a step\n",
    "    total_reward += reward\n",
    "    el += 1\n",
    "    if done or el == ep_limit:  # If done == True, the episode has ended\n",
    "        break\n",
    "    state = next_state\n",
    "    \n",
    "print(\"Reward of Random Policy:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1:  Q-Learning\n",
    "Your first assignment is to implement the Q-Learning algorithm on **Cliff**. In particular you need to implement both $\\epsilon$-greedy and Softmax versions for the exploration heuristic. The solution returned must be a tuple (policy, rewards, lengths) where:\n",
    "\n",
    "- *policy*: array of action identifiers where the $i$-th action refers to the $i$-th state\n",
    "- *rewards*: array of rewards where the $i$-th reward refers to the $i$-th episode of the training performed\n",
    "- *lengths*: array of lengths where the $i$-th length refers to the $i$-th episode of the training performed (length in number of steps)\n",
    "\n",
    "Implemented exploration functions:\n",
    "- *epsilon_greedy(q, state, epsilon)*\n",
    "- *softmax(softmax(q, state, temp)*\n",
    "\n",
    "**Functions to implement:**\n",
    "- *q_learning(environment, episodes, alpha, gamma, expl_func, expl_param)*\n",
    "\n",
    "It could be useful to draw a number given a specific probability distribution. In particular, among the 5 choices, the 3rd is the one that is most likely to be chosen (highest probability value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(5, p=[0.1, 0.2, 0.5, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following exploration functions could be used in the algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(q.shape[1])\n",
    "    return q[state].argmax()\n",
    "\n",
    "def softmax(q, state, temp):\n",
    "    \"\"\"\n",
    "    Softmax action selection function\n",
    "    \n",
    "    Args:\n",
    "    q: q table\n",
    "    state: agent's current state\n",
    "    temp: temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    e = np.exp(q[state] / temp)\n",
    "    return np.random.choice(q.shape[1], p=e / e.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that since the executions are stochastic, the policy could differ: the important thing is that the policy obtained using Q-Learning chooses the shortest path toward the goal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Cliff-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Q-Learning epsilon greedy\n",
    "policy, rewards, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(f\"Execution time: {round(timer() - t, 4)}s\") \n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.shape))\n",
    "_ = run_episode(env, policy, 20)\n",
    "\n",
    "results = CheckResult_L4(policy_render)\n",
    "results.check_qlearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: SARSA\n",
    "Your second assignment is to implement the SARSA algorithm on **Cliff**. In particular you need to implement both $\\epsilon$-greedy and Softmax versions for the exploration heuristic (you can reuse the same functions of Assignment 1). The solution returned must be a tuple (policy, rewards, lengths) where:\n",
    "\n",
    "- *policy*: array of action identifiers where the $i$-th action refers to the $i$-th state\n",
    "- *rewards*: array of rewards where the $i$-th reward refers to the $i$-th episode of the training performed\n",
    "- *lengths*: array of lengths where the $i$-th length refers to the $i$-th episode of the training performed (length in number of steps)\n",
    "\n",
    "**Functions to implement:**\n",
    "\n",
    "- *SARSA(environment, episodes, alpha, gamma, expl_func, expl_param)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that since the executions are stochastic, the policy could differ: the important thing is that the policy obtained using SARSA chooses the longer but safer path toward the goal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"Cliff-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: {} \\n\\tSARSA\".format(envname))\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# SARSA epsilon greedy\n",
    "policy, rews, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(f\"Execution time: {round(timer() - t, 4)}s\") \n",
    "_ = run_episode(env, policy, 20)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.shape))\n",
    "\n",
    "\n",
    "results = CheckResult_L4(policy_render)\n",
    "results.check_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Now that you have veryfied the results, try to employ Softmax instead of $\\epsilon$-greedy as exploration heuristic. Are there any significant changes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "\n",
    "The following code performs a comparison between the 2 reinforcement learning algorithms: *Q-Learning* and *SARSA*. Execute the following code and analyze the charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"Cliff-v0\"\n",
    "print()\n",
    "print(\"###########################################################\")\n",
    "print(\"################ Environment: Cliff-v0 ####################\")\n",
    "print(\"###########################################################\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 600\n",
    "ep_limit = 50\n",
    "vmaxiters = 50\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "delta = 1e-3\n",
    "\n",
    "rewser = []\n",
    "lenser = []\n",
    "\n",
    "litres = np.arange(1, episodes + 1)  # Learning iteration values\n",
    "window = 50  # Rolling window\n",
    "mrew = np.zeros(episodes)\n",
    "mlen = np.zeros(episodes)\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Q-Learning\n",
    "_, rews, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "\n",
    "# SARSA\n",
    "_, rews, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"label\": \"SARSA\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"label\": \"SARSA\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "\n",
    "plot(rewser, \"Rewards\", \"Episodes\", \"Rewards\")\n",
    "plot(lenser, \"Lengths\", \"Episodes\", \"Lengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend.\n",
    "\n",
    "**Algorithms Reward comparison**\n",
    "<img src=\"images/results-reward.png\" width=\"600\">\n",
    "\n",
    "**Algorithms Episode Length comparison**\n",
    "<img src=\"images/results-length.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
